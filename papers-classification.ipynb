{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec5ded02-3424-49f4-92d5-ebfa7dcb694b",
   "metadata": {},
   "source": [
    "# Papers classification\n",
    "\n",
    "***Notre objectif est de construire un modèle qui utilise le résumé et le titre d'un article pour prédire s'il sera rejeté ou non.***\n",
    "\n",
    "C'est un article avec du code pour affiner BERT afin d'effectuer une classification de texte sur un ensemble de données d'articles scientifique acceptés et rejétés.\n",
    "\n",
    "Dans cet article, nous allons :\n",
    "\n",
    "- Charger le jeu de données Papers-Dataset\n",
    "- Charger un modèle BERT à partir de [Huggingface](https://huggingface.co/)\n",
    "- Construire notre propre modèle en combinant BERT avec un classificateur\n",
    "- Entraîner le modèle en rafinnant BERT dans le cadre de notre tâche\n",
    "- Enregister le modèle afin de l'utiliser pour classer des articles\n",
    "\n",
    "A la fin, vous aurez une architecture que vous pouvez réutiliser dans vos prochains projets de classifications de texte.\n",
    "\n",
    "### C'est quoi BERT ?\n",
    "\n",
    "Introduit en 2018, [BERT: Bidirectional Encoder Representations from Transformers](https://arxiv.org/abs/1810.04805), selon ses auteurs est conçu pour pré-entraîner des représentations bidirectionnelles profondes à partir de textes non étiquetés en conditionnant **conjointement le contexte gauche et droit** dans toutes les couches.\n",
    "\n",
    "BERT est survenu pour compléter deux techniques de word embedding: [ELMo](https://paperswithcode.com/method/elmo) et [GPT](https://paperswithcode.com/method/gpt).\n",
    "Pendant que ELMo encode le contexte de manière bidirectionnelle mais utilise des architectures spécifiques aux tâches GPT est indifférent aux tâches mais code le contexte de gauche à droite.\n",
    "\n",
    "Nous pouvons résumer les caractéristiques de ces modèles comme suit:\n",
    "\n",
    "| Modèle | Context                | Tâche                  | Encode           |\n",
    "| ------ | ---------------------- | ---------------------- | ---------------- |\n",
    "| ELMo   | sensible au contexte ✅ | spécifique à la tâche  | bi-directionnel✅ |\n",
    "| GPT    | sensible au contexte   | agnostique à la tâche✅ | gauche à droite  |\n",
    "| BERT   | sensible au contexte   | agnostique à la tâche  | bi-directionnel  |\n",
    "\n",
    "### Outils et pré-requits\n",
    "Pour construire notre modèle, nous travaillerons avec le framework Pytorch et [Pytorch Lightning](https://www.pytorchlightning.ai/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef5f70a-4b7f-4ab2-a924-a945f58eedf6",
   "metadata": {},
   "source": [
    "## 1- Preparation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2234c064-8761-4308-bd14-f5f14e8f38b1",
   "metadata": {},
   "source": [
    "### 1-1 Chargement des données\n",
    "\n",
    "Vous pouvez trouver le jeu de données utilisé à cette [adresse](https://raw.githubusercontent.com/Godwinh19/Papers-Dataset/main/data/ICLR%20papers%20datasets.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "519ba31b-8c5b-4b39-9508-f831b461ab6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import io\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed52a1b6-16fd-44df-bf4e-3bc144867a5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>accepted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What Matters for On-Policy Deep Actor-Critic M...</td>\n",
       "      <td>In recent years, reinforcement learning (RL) h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Theoretical Analysis of Self-Training with Dee...</td>\n",
       "      <td>Self-training algorithms, which train a model ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Learning to Reach Goals via Iterated Supervise...</td>\n",
       "      <td>Current reinforcement learning (RL) algorithms...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Deep symbolic regression: Recovering mathemati...</td>\n",
       "      <td>Discovering the underlying mathematical expres...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Optimal Rates for Averaged Stochastic Gradient...</td>\n",
       "      <td>We analyze the convergence of the averaged sto...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  What Matters for On-Policy Deep Actor-Critic M...   \n",
       "1  Theoretical Analysis of Self-Training with Dee...   \n",
       "2  Learning to Reach Goals via Iterated Supervise...   \n",
       "3  Deep symbolic regression: Recovering mathemati...   \n",
       "4  Optimal Rates for Averaged Stochastic Gradient...   \n",
       "\n",
       "                                            abstract  accepted  \n",
       "0  In recent years, reinforcement learning (RL) h...         1  \n",
       "1  Self-training algorithms, which train a model ...         1  \n",
       "2  Current reinforcement learning (RL) algorithms...         1  \n",
       "3  Discovering the underlying mathematical expres...         1  \n",
       "4  We analyze the convergence of the averaged sto...         1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_url = \"https://raw.githubusercontent.com/Godwinh19/Papers-Dataset/main/data/ICLR%20papers%20datasets.csv\"\n",
    "s = requests.get(dataset_url).content\n",
    "data = pd.read_csv(io.StringIO(s.decode('utf-8')), usecols=['title', 'abstract', 'accepted'])\n",
    "# Affichons l'entête de nos données\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f847ef0-536b-4a3e-a608-42a13b075fc8",
   "metadata": {},
   "source": [
    "Dans ce tableau, nous avons:\n",
    "- title: le titre de l'article\n",
    "- abstract: le résumé\n",
    "- accepted: champ décrivant si l'article a été accépté (1) ou non (0)\n",
    "\n",
    "Dans notre cas, nous allons nous intéresser aux champs *title, abstract, accepted*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1813315d-7244-472c-9c3a-e1006ad608d2",
   "metadata": {},
   "source": [
    "### 1-2 Transformation des colonnes\n",
    "\n",
    "Nous allons transformer les colonnes *title* et *abstract* en une seule colonne appélé *description*; puis renommer le champ *accepted* en *label* de par sa fonction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fe8d015-8f5d-49d6-82ae-f8ff93ce86b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What Matters for On-Policy Deep Actor-Critic M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Theoretical Analysis of Self-Training with Dee...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Learning to Reach Goals via Iterated Supervise...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Deep symbolic regression: Recovering mathemati...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Optimal Rates for Averaged Stochastic Gradient...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  label\n",
       "0  What Matters for On-Policy Deep Actor-Critic M...      1\n",
       "1  Theoretical Analysis of Self-Training with Dee...      1\n",
       "2  Learning to Reach Goals via Iterated Supervise...      1\n",
       "3  Deep symbolic regression: Recovering mathemati...      1\n",
       "4  Optimal Rates for Averaged Stochastic Gradient...      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['description'] = data['title'] + \" - \" + data['abstract']\n",
    "transformed_data = data[['description', 'accepted']].rename(columns={'accepted': 'label'}).copy()\n",
    "transformed_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9e8a46-dec8-461c-a32d-506949ecaa22",
   "metadata": {},
   "source": [
    "### 1-3 Transformation des données en entrée au modèle\n",
    "\n",
    "> Le code pour le traitement des échantillons de données peut devenir désordonné et difficile à maintenir ; idéalement, nous voulons que le code de nos ensembles de données soit découplé du code d'apprentissage de nos modèles pour une meilleure lisibilité et modularité.\n",
    ">\n",
    "> <cite>pytorch docs</cite>\n",
    "\n",
    "Avec pytorch, nous chargeons les données avec la classe `Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d02a33b-edd3-4346-a978-cc56f02a8f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class PapersDataset(Dataset):\n",
    "    def __init__(self, description, targets, tokenizer, max_length):\n",
    "        self.description = description\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.description)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        description = str(self.description[item])\n",
    "        target = self.targets[item]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            description,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding=\"max_length\",\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"article_text\": description,\n",
    "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "            \"targets\": torch.tensor(target, dtype=torch.long),\n",
    "        }\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dd9c4b-a782-4cf4-b362-129ba00e5810",
   "metadata": {},
   "source": [
    "Précedemment, nous avons introduit `tokenizer`. De façon simple, la tokénisation des mots est le processus qui consiste à diviser un grand échantillon de texte en mots. Il s'agit d'une exigence fondamentale dans les tâches de traitement du langage naturel où chaque mot doit être capturé séparément pour une analyse ultérieure. [Lire sur la tokenisation ici](https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959141af-7be9-4191-97b6-c55bd00d148c",
   "metadata": {},
   "source": [
    "## 2- Chargement des données dans des dataloaders\n",
    "\n",
    "Toujour dans l'objectif de rendre le code ordonné et facile à maintenir, nous allons faire une dernière transformation qui consiste en charger les données dans des `dataloader`. Pour faire ces configurations, nous allons utiliser *pytorch lightning*\n",
    "Pous plus de détails, veuillez lire la documentation du module de données de lightning qui explique chaque étape du processus [ici](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.core.datamodule.html#pytorch_lightning.core.datamodule.LightningDataModule)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed15c4fd-45e5-40b5-9b8f-b7e3b111b9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40b75a3b-cd06-4325-9f49-1a1748ca0812",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialization of inherited lightning data module\n",
    "        \"\"\"\n",
    "        super(BertDataModule, self).__init__()\n",
    "        self.BERT_PRE_TRAINED_MODEL_NAME = \"bert-base-uncased\"\n",
    "        self.df_train = None\n",
    "        self.df_val = None\n",
    "        self.df_test = None\n",
    "        \n",
    "        self.train_data_loader = None\n",
    "        self.val_data_loader = None\n",
    "        self.test_data_loader = None\n",
    "        \n",
    "        self.MAX_LEN = 100\n",
    "        self.encoding = None\n",
    "        self.tokenizer = None\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        \"\"\"\n",
    "        Read the data, parse it and split the data into train, test, validation data\n",
    "\n",
    "        :param stage: Stage - training or testing\n",
    "        \"\"\"\n",
    "        \n",
    "        num_samples = 80\n",
    "        df = (\n",
    "            transformed_data\n",
    "            .sample(num_samples)\n",
    "        )\n",
    "        \n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.BERT_PRE_TRAINED_MODEL_NAME)\n",
    "        \n",
    "        RANDOM_SEED = 0\n",
    "        np.random.seed(RANDOM_SEED)\n",
    "        torch.manual_seed(RANDOM_SEED)\n",
    "        \n",
    "        df_train, df_test = train_test_split(\n",
    "            df, test_size=0.3, random_state=RANDOM_SEED, stratify=df[\"label\"]\n",
    "        )\n",
    "        \n",
    "        df_val, df_test = train_test_split(\n",
    "            df_test, test_size=0.5, random_state=RANDOM_SEED, stratify=df_test[\"label\"]\n",
    "        )\n",
    "        \n",
    "        self.df_train, self.df_val, self.df_test = df_train, df_val, df_test\n",
    "    \n",
    "    def create_data_loader(self, df, tokenizer, max_len, batch_size=8):\n",
    "        \"\"\"\n",
    "        Generic data loader function\n",
    "\n",
    "        :param df: Input dataframe\n",
    "        :param tokenizer: bert tokenizer\n",
    "        :param max_len: Max length of the claims datapoint\n",
    "        :param batch_size: Batch size for training\n",
    "\n",
    "        :return: Returns the constructed dataloader\n",
    "        \"\"\"\n",
    "        dataset = PapersDataset(\n",
    "            description=df.description.to_numpy(),\n",
    "            targets=df.label.to_numpy(),\n",
    "            tokenizer=tokenizer,\n",
    "            max_length=max_len\n",
    "        )\n",
    "        \n",
    "        return DataLoader(\n",
    "            dataset, batch_size=batch_size, num_workers=0\n",
    "        )\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        \"\"\"\n",
    "        :return: output - Train data loader for the given input\n",
    "        \"\"\"\n",
    "        self.train_data_loader = self.create_data_loader(\n",
    "            self.df_train, self.tokenizer, self.MAX_LEN \n",
    "        )\n",
    "        \n",
    "        return self.train_data_loader\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        \"\"\"\n",
    "        :return: output - Validation data loader for the given input\n",
    "        \"\"\"\n",
    "        self.val_data_loader = self.create_data_loader(\n",
    "            self.df_val, self.tokenizer, self.MAX_LEN\n",
    "        )\n",
    "        return self.val_data_loader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        \"\"\"\n",
    "        :return: output - Test data loader for the given input\n",
    "        \"\"\"\n",
    "        self.test_data_loader = self.create_data_loader(\n",
    "            self.df_test, self.tokenizer, self.MAX_LEN\n",
    "        )\n",
    "        return self.test_data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d6bf2f-4d18-46c7-a2c4-33bb936f4d75",
   "metadata": {},
   "source": [
    "## 3- Construction du réseau\n",
    "Dans cette étape, nous allons construire notre classifier à partir d'un modèle appris de BERT.\n",
    "\n",
    "La configuration d'un modele avec pytorch lightning est expliqué [ici](https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1f87206-d32f-4e25-81ce-c4825fef6f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pytorch_lightning.callbacks import (\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    "    LearningRateMonitor,\n",
    ")\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch import nn\n",
    "from transformers import BertModel, AdamW\n",
    "\n",
    "class BertPapersClassifier(pl.LightningModule):\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Initializes the network, optimizer and scheduler\n",
    "        \"\"\"\n",
    "        super(BertPapersClassifier, self).__init__()\n",
    "        self.BERT_PRE_TRAINED_MODEL_NAME = \"bert-base-uncased\"\n",
    "        \n",
    "        self.bert_model = BertModel.from_pretrained(self.BERT_PRE_TRAINED_MODEL_NAME)\n",
    "    \n",
    "        \n",
    "        for param in self.bert_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.drop = nn.Dropout(p=0.2)\n",
    "        n_classes = 2\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.bert_model.config.hidden_size, 512)\n",
    "        self.out = nn.Linear(512, n_classes)\n",
    "        \n",
    "        self.scheduler = None\n",
    "        self.optimizer = None\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        :param input_ids: Input data\n",
    "        :param attention_maks: Attention mask value\n",
    "\n",
    "        :return: output - Accepted or not for the given papers snippet\n",
    "        \"\"\"\n",
    "        output = self.bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        output = F.relu(self.fc1(output.pooler_output))\n",
    "        output = self.drop(output)\n",
    "        output = self.out(output)\n",
    "        return output\n",
    "    \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Training the data as batches and returns training loss on each batch\n",
    "\n",
    "        :param train_batch Batch data\n",
    "        :param batch_idx: Batch indices\n",
    "\n",
    "        :return: output - Training loss\n",
    "        \"\"\"\n",
    "        input_ids = train_batch[\"input_ids\"].to(self.device)\n",
    "        attention_mask = train_batch[\"attention_mask\"].to(self.device)\n",
    "        targets = train_batch[\"targets\"].to(self.device)\n",
    "        \n",
    "        output = self.forward(input_ids, attention_mask)\n",
    "        loss = F.cross_entropy(output, targets)\n",
    "        self.log(\"train loss\", loss)\n",
    "        return {\"loss\": loss}\n",
    "    \n",
    "    def test_step(self, test_batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Performs test and computes the accuracy of the model\n",
    "\n",
    "        :param test_batch: Batch data\n",
    "        :param batch_idx: Batch indices\n",
    "\n",
    "        :return: output - Testing accuracy\n",
    "        \"\"\"\n",
    "        input_ids = test_batch[\"input_ids\"].to(self.device)\n",
    "        attention_mask = test_batch[\"attention_mask\"].to(self.device)\n",
    "        targets = test_batch[\"targets\"].to(self.device)\n",
    "        \n",
    "        output = self.forward(input_ids, attention_mask)\n",
    "        _, y_hat = torch.max(output, dim=1)\n",
    "        test_acc = accuracy_score(y_hat.cpu(), targets.cpu())\n",
    "        \n",
    "        return {\"test_acc\": torch.tensor(test_acc)}\n",
    "    \n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Performs validation of data in batches\n",
    "\n",
    "        :param val_batch: Batch data\n",
    "        :param batch_idx: Batch indices\n",
    "\n",
    "        :return: output - valid step loss\n",
    "        \"\"\"\n",
    "\n",
    "        input_ids = val_batch[\"input_ids\"].to(self.device)\n",
    "        attention_mask = val_batch[\"attention_mask\"].to(self.device)\n",
    "        targets = val_batch[\"targets\"].to(self.device)\n",
    "        \n",
    "        output = self.forward(input_ids, attention_mask)\n",
    "        loss =  F.cross_entropy(output, targets)\n",
    "        \n",
    "        return {\"val_step_loss\": loss}\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        \"\"\"\n",
    "        Computes average validation accuracy\n",
    "\n",
    "        :param outputs: outputs after every epoch end\n",
    "\n",
    "        :return: output - average valid loss\n",
    "        \"\"\"\n",
    "        avg_loss = torch.stack([x[\"val_step_loss\"] for x in outputs]).mean()\n",
    "        self.log(\"val_loss\", avg_loss, sync_dist=True)\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        \"\"\"\n",
    "        Computes average test accuracy score\n",
    "\n",
    "        :param outputs: outputs after every epoch end\n",
    "\n",
    "        :return: output - average test loss\n",
    "        \"\"\"\n",
    "        avg_test_acc = torch.stack([x[\"test_acc\"] for x in outputs]).mean()\n",
    "        self.log(\"avg_test_acc\", avg_test_acc)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        Initializes the optimizer and learning rate scheduler\n",
    "\n",
    "        :return: output - Initialized optimizer and scheduler\n",
    "        \"\"\"\n",
    "        self.optimizer = AdamW(self.parameters(), lr=0.001)\n",
    "        self.scheduler = {\n",
    "            \"scheduler\": torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                self.optimizer,\n",
    "                mode=\"min\",\n",
    "                factor=0.2,\n",
    "                patience=2,\n",
    "                min_lr=1e-6,\n",
    "                verbose=True\n",
    "            ),\n",
    "            \"monitor\": \"val_loss\",\n",
    "        }\n",
    "        return [self.optimizer], [self.scheduler]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c136256-6cf6-41df-b43c-f0fd0ba8623a",
   "metadata": {},
   "source": [
    "## 4- Entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88002b89-2b00-4eb8-8f7c-2824f4bc3e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "C:\\Users\\RD_3\\anaconda3\\envs\\torch\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:147: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "C:\\Users\\RD_3\\anaconda3\\envs\\torch\\lib\\site-packages\\pytorch_lightning\\core\\datamodule.py:469: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
      "  rank_zero_deprecation(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Missing logger folder: C:\\Users\\RD_3\\Documents\\Python\\PapersDataset\\Papers-classification\\lightning_logs\n",
      "\n",
      "  | Name       | Type      | Params\n",
      "-----------------------------------------\n",
      "0 | bert_model | BertModel | 109 M \n",
      "1 | drop       | Dropout   | 0     \n",
      "2 | fc1        | Linear    | 393 K \n",
      "3 | out        | Linear    | 1.0 K \n",
      "-----------------------------------------\n",
      "394 K     Trainable params\n",
      "109 M     Non-trainable params\n",
      "109 M     Total params\n",
      "439.508   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14c85ec5303d4ad4a01bc03e3d178705",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 0.783\n",
      "Epoch 0, global step 6: val_loss reached 0.78308 (best 0.78308), saving model to \"C:\\Users\\RD_3\\Documents\\Python\\PapersDataset\\Papers-classification\\epoch=0-step=6.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.127 >= min_delta = 0.0. New best score: 0.656\n",
      "Epoch 1, global step 13: val_loss reached 0.65584 (best 0.65584), saving model to \"C:\\Users\\RD_3\\Documents\\Python\\PapersDataset\\Papers-classification\\epoch=1-step=13.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 20: val_loss was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 27: val_loss was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.026 >= min_delta = 0.0. New best score: 0.630\n",
      "Epoch 4, global step 34: val_loss reached 0.63005 (best 0.63005), saving model to \"C:\\Users\\RD_3\\Documents\\Python\\PapersDataset\\Papers-classification\\epoch=4-step=34.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.033 >= min_delta = 0.0. New best score: 0.597\n",
      "Epoch 5, global step 41: val_loss reached 0.59723 (best 0.59723), saving model to \"C:\\Users\\RD_3\\Documents\\Python\\PapersDataset\\Papers-classification\\epoch=5-step=41.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 48: val_loss was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 55: val_loss was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_loss did not improve in the last 3 records. Best score: 0.597. Signaling Trainer to stop.\n",
      "Epoch 8, global step 62: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     9: reducing learning rate of group 0 to 2.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at C:\\Users\\RD_3\\Documents\\Python\\PapersDataset\\Papers-classification\\epoch=5-step=41.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from checkpoint at C:\\Users\\RD_3\\Documents\\Python\\PapersDataset\\Papers-classification\\epoch=5-step=41.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2257adb8e58b48b6a1b000c4afa74747",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'avg_test_acc': 0.6875}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "data_module = BertDataModule(accelerator=\"gpu\")\n",
    "data_module.setup(stage=\"fit\")\n",
    "\n",
    "b_model = BertPapersClassifier()\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", mode=\"min\", verbose=True)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=os.getcwd(),\n",
    "        save_top_k=1,\n",
    "        verbose=True,\n",
    "        monitor=\"val_loss\",\n",
    "        \n",
    "        mode=\"min\",\n",
    "    )\n",
    "lr_logger = LearningRateMonitor()\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10, gpus=1, accelerator=\"gpu\",\n",
    "    callbacks=[lr_logger, early_stopping, checkpoint_callback], checkpoint_callback=True,\n",
    ")\n",
    "\n",
    "trainer.fit(b_model, data_module)\n",
    "trainer.test(datamodule=data_module)\n",
    "\n",
    "torch.save(b_model.state_dict(), \"bert_model_dict.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db3e49c-2fb9-4d99-9a37-3d11254dd0a3",
   "metadata": {},
   "source": [
    "## 5- Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1c803ce-0857-4a9a-82c8-a297ce125301",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertPapersClassifier(\n",
      "  (bert_model): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (drop): Dropout(p=0.2, inplace=False)\n",
      "  (fc1): Linear(in_features=768, out_features=512, bias=True)\n",
      "  (out): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n",
      "tensor([[  101, 20288, 24110, 23355, 17435,  9324,  2075,  1011,  2445,  1037,\n",
      "          2312,  2951,  8185,  1010, 12403,  2869, 11787,  1010, 24110,  3775,\n",
      "          6774,  1010,  1998,  1013,  2030,  4488,  2060,  4443,  1011,  7968,\n",
      "         27400,  3136,  2064,  2031,  3365,  6666,  1010,  7478,  2013, 21485,\n",
      "          2039,  2009, 25284, 13792,  2005,  4563, 15973,  7399, 11208,  3471,\n",
      "          2000,  4346, 27400, 17736,  2000,  2640,  2110,  1011,  1997,  1011,\n",
      "          1996,  1011,  2396, 15756,  2897,  4275,  1012,  2182,  1010,  2057,\n",
      "         18077,  5906,  2013,  6721,  8185,  3399,  2000,  2191, 10480,  8635,\n",
      "          2055,  2129,  1996,  1041, 29206, 13102, 22471,  6824,  1997,  1037,\n",
      "          8185,  3431,  2104,  2107, 27400, 21865,  1012,  1999,  3327,  1010,\n",
      "          2057,  2265,  2008,  2200,  2210,  2689,  5158,  1999,  1996, 12367,\n",
      "          8082,  1041, 29206,  3367,  6820, 14890,  1010,  2130,  2104, 23956,\n",
      "         12403,  2869,  9031,  1013, 24110,  3775,  9276,  1010,  1998,  8821,\n",
      "          2008,  2200,  2210, 13248,  2836,  3279,  5158,  2043,  2551,  2007,\n",
      "          2200, 24663, 12403,  2869,  7810,  2030, 24110, 23355, 17435,  9324,\n",
      "          2075,  3471,  1012,  2057, 19141,  2129,  2122,  3463, 12530,  2006,\n",
      "          1996, 27400,  3012,  1010,  2057,  2839,  4697,  1037,  4403,  6653,\n",
      "          3458,  2029, 17435,  9324,  2075,  4150,  2825,  1010,  1998,  2057,\n",
      "          2265,  2043,  2107, 27400, 21865,  2064,  8970, 12996,  6313,  2512,\n",
      "          1011, 12367,  8082,  1041, 29206,  3726, 24817,  1012,   102]])\n",
      "tensor([[-0.2612, -0.0729]], grad_fn=<AddmmBackward0>)\n",
      "torch.return_types.max(\n",
      "values=tensor([-0.0729]),\n",
      "indices=tensor([1]))\n",
      "tensor([True])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "article = \"Sparse Quantized Spectral Clustering - Given a large data matrix, sparsifying, quantizing, and/or performing other entry-wise nonlinear operations can have numerous benefits, ranging from speeding up iterative algorithms for core numerical linear algebra problems to providing nonlinear filters to design state-of-the-art neural network models. Here, we exploit tools from random matrix theory to make precise statements about how the eigenspectrum of a matrix changes under such nonlinear transformations. In particular, we show that very little change occurs in the informative eigenstructure, even under drastic sparsification/quantization, and consequently that very little downstream performance loss occurs when working with very aggressively sparsified or quantized spectral clustering problems.\\\n",
    "We illustrate how these results depend on the nonlinearity, we characterize a phase transition beyond which spectral clustering becomes possible, and we show when such nonlinear transformations can introduce spurious non-informative eigenvectors.\"\n",
    "#original label = 1 : accepted\n",
    "\n",
    "# Predict on a Pandas DataFrame.\n",
    "import pandas as pd\n",
    "\n",
    "model = BertPapersClassifier()\n",
    "\n",
    "model.load_state_dict(torch.load(\"bert_model_dict.pt\"))\n",
    "model.eval()\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "inputs = tokenizer(article, padding=True)\n",
    "\n",
    "input_ids = torch.tensor(inputs[\"input_ids\"]).unsqueeze(0)\n",
    "attention_mask = torch.tensor(inputs[\"attention_mask\"]).unsqueeze(0)\n",
    "\n",
    "\n",
    "print(model)\n",
    "print(input_ids)\n",
    "\n",
    "out = model(input_ids, attention_mask)\n",
    "print(out)\n",
    "print(torch.max(out.data, 1))\n",
    "print(torch.max(out.data, 1).indices==torch.tensor([1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edaee43-5429-4092-9dbc-13b740f7a11c",
   "metadata": {},
   "source": [
    "En prenant l'exemple de [cet article](https://openreview.net/forum?id=pBqLS-7KYAF) qui n'était pas dans nos données, notre modèle retourne une acceptation 😁:\n",
    "\n",
    "## Note de fin\n",
    "\n",
    "L'ensemble du code est disponible [ici](https://github.com/Godwinh19/Papers-Dataset/)\n",
    "Notre jeu de données utilisé est très faible, nous allons l'augmenter mais toute fois notre modèle accomplit sa fonction. \n",
    "Dans un prochain article, nous verrons comment nous pouvons effectuer le monitoring de nos modèles, mettre en production les plus performants tout en ayant les meilleurs paramètres.\n",
    "\n",
    "Cheers ☕!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623c2200-2592-40d9-bef1-75022a50086a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
